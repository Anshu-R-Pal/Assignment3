{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgaDrpoVYcd28Ei5a5yZpm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sIcivTreLrm-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BbnDZqLSY8zE","executionInfo":{"status":"ok","timestamp":1765671519067,"user_tz":-330,"elapsed":3669,"user":{"displayName":"Anshu Pal cs24m506","userId":"14280763061844336089"}}},"outputs":[],"source":["\n","# utils.py\n","import torch\n","import torch.nn as nn\n","\n","# ============================================================\n","# Q1(c): Activation memory (Conv layers only)\n","# ============================================================\n","activation_memory = {}\n","\n","def activation_hook(name):\n","    def hook(module, input, output):\n","        if isinstance(output, torch.Tensor):\n","            activation_memory[name] = {\n","                \"num_elements\": output.numel(),\n","                \"shape\": tuple(output.shape)\n","            }\n","    return hook\n","\n","\n","def register_activation_hooks(model):\n","    \"\"\"\n","    Register hooks ONLY on Conv2d layers (as expected in Q1c).\n","    \"\"\"\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Conv2d):\n","            module.register_forward_hook(activation_hook(name))\n","\n","\n","# ============================================================\n","# Professor-style n-bit Post-Training Quantization (PTQ)\n","# ============================================================\n","def qparams_from_minmax(xmin, xmax, n_bits=8, eps=1e-12):\n","    \"\"\"\n","    Symmetric per-tensor quantization parameters.\n","    \"\"\"\n","    qmax = (1 << (n_bits - 1)) - 1\n","    max_abs = torch.max(xmin.abs(), xmax.abs()).clamp_min(eps)\n","    scale = max_abs / qmax\n","    return scale, qmax\n","\n","\n","def quantize_tensor(x, scale, qmax):\n","    q = torch.round(x / scale)\n","    q = torch.clamp(q, -qmax, qmax)\n","    return q * scale\n","\n","\n","def quantize_model_weights(model, bits=8):\n","    \"\"\"\n","    Post-training fake quantization of weights.\n","    \"\"\"\n","    with torch.no_grad():\n","        for name, param in model.named_parameters():\n","            if \"weight\" in name:\n","                xmin, xmax = param.min(), param.max()\n","                scale, qmax = qparams_from_minmax(xmin, xmax, bits)\n","                param.copy_(quantize_tensor(param, scale, qmax))\n","\n","\n","# ============================================================\n","# Model size (analytical â€” required for Q2/Q3)\n","# ============================================================\n","def get_model_size(model, bits=32):\n","    \"\"\"\n","    Analytical model size in MB.\n","    Weights use 'bits', biases remain FP32.\n","    \"\"\"\n","    total_bits = 0\n","    for name, p in model.named_parameters():\n","        if \"weight\" in name:\n","            total_bits += p.numel() * bits\n","        elif \"bias\" in name:\n","            total_bits += p.numel() * 32\n","    return total_bits / 8 / (1024 ** 2)\n"]}]}